{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Flask Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from itertools import product\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_type, data):\n",
    "    if model_type == 'AR':\n",
    "        return AR_model(data)\n",
    "    elif model_type == 'ARIMA':\n",
    "        return ARIMA_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(ABC):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Abstract method to train the model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forecast(self, forecast_days):\n",
    "        \"\"\"\n",
    "        Abstract method to make predictions using the trained model.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 AR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AR_model(Model):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "        self.data = data\n",
    "        self.trained_model = None\n",
    "        self.model_type = 'AR'\n",
    "\n",
    "    def train(self):\n",
    "        # Handle NaNs\n",
    "        data = self.data.dropna(subset=['Close'])\n",
    "\n",
    "        # Define parameter grid for tuning\n",
    "        trends = ['n', 'c', 't', 'ct']\n",
    "        min_lag = 1\n",
    "        max_lag = len(data) \n",
    "        lags_range = range(min_lag, max_lag) \n",
    "\n",
    "        best_r2 = -float('inf') \n",
    "        best_params = None\n",
    "\n",
    "        # Perform grid search with cross-validation on the training set\n",
    "        # Choose the best params based on R2 score\n",
    "        n_splits = 3\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)  # Time series cross-validation\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        for trend, lags in product(trends, lags_range):\n",
    "            for train_index, val_index in tscv.split(data):\n",
    "                train_split, val_split = data.iloc[train_index], data.iloc[val_index]\n",
    "                try:\n",
    "                    model = AutoReg(train_split['Close'].values, lags=lags, trend=trend).fit()\n",
    "                    predictions = model.predict(start=len(train_split), end=len(train_split) + len(val_split) - 1)\n",
    "                    r2 = r2_score(val_split['Close'], predictions)\n",
    "                    r2_sum += r2\n",
    "                except Exception as e:\n",
    "                    print(f\"Error for trend={trend}, lags={lags}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Average R2 score across folds\n",
    "            avg_r2 = r2_sum / n_splits\n",
    "            \n",
    "            # Update best parameters if better R2 found\n",
    "            if avg_r2 > best_r2:\n",
    "                best_r2 = avg_r2\n",
    "                best_params = (trend, lags)\n",
    "\n",
    "        best_trend, best_lags = best_params\n",
    "        \n",
    "        print(f\"Best R2 score: {best_r2:.4f}\")\n",
    "        print(f\"Best parameters: trend={best_params[0]}, lags={best_params[1]}\")\n",
    "        \n",
    "        # Fit the best model on the entire dataset \n",
    "        try:\n",
    "            self.trained_model = AutoReg(data['Close'].values, lags=best_lags, trend=best_trend).fit()\n",
    "        except Exception as e:\n",
    "            print('Model training failed with the error message: {e}')\n",
    "        \n",
    "        return self.trained_model\n",
    "    \n",
    "    def forecast(self, forecast_days):\n",
    "        #Forecast next forecast_period days\n",
    "        start = len(self.data)\n",
    "        end = start + forecast_days - 1\n",
    "        forecast_prices = self.trained_model.predict(start=start, end=end)\n",
    "\n",
    "        ''' PLOTTING\n",
    "        # Create a date range for the predictions\n",
    "        last_date = data.index[-1]\n",
    "        prediction_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_period)\n",
    "\n",
    "        # Plot the results\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(data.index, data['Close'], label='Historical Data', color='black')\n",
    "        plt.plot(prediction_dates, forecast_predictions, label='Forecast', linestyle='-', color='red', alpha = 0.7)\n",
    "\n",
    "        # Ensure the first date from data.index and last date from prediction_dates are on the x-axis\n",
    "        plt.title(f'{self.model_type} Model Prediction of Close Prices')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Close Price')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        '''\n",
    "\n",
    "        return forecast_prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Database for User and Model Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask_sqlalchemy import SQLAlchemy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql://username:password@localhost/dbname'\n",
    "db = SQLAlchemy(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Users Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Users(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    user_id = db.Column(db.String(50), unique=True, nullable=False)\n",
    "    email = db.Column(db.String(120), unique=True, nullable=False)\n",
    "    password = db.Column(db.String(255), nullable=False)\n",
    "    account_type = db.Column(db.String(20), nullable=False)  # basic or premium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of adding a new user\n",
    "new_user = User(user_id='12345', email='example@email.com', password='hashed_password', account_type='premium')\n",
    "db.session.add(new_user)\n",
    "db.session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Trained Models Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainedModels(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    symbol = db.Column(db.String(20), nullable=False)\n",
    "    model_type = db.Column(db.String(50), nullable=False)\n",
    "    start_date = db.Column(db.String(50), nullable=False)\n",
    "    end_date = db.Column(db.String(50), nullable=False)  # Nullable if model is ongoing\n",
    "    trained_model = db.Column(db.Text)  # Serialized model data or file path\n",
    "\n",
    "    def save_trained_model(self, trained_model):\n",
    "        \"\"\"\n",
    "        Save the trained model to the database.\n",
    "        \"\"\"\n",
    "        self.trained_model = pickle.dumps(trained_model)\n",
    "        db.session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_model(model_type, start_date, end_date, symbol):\n",
    "    model = TrainedModels.query.filter_by(model_type=model_type,\n",
    "                                          start_date=start_date,\n",
    "                                          end_date=end_date,\n",
    "                                          symbol=symbol).first()\n",
    "    if model:\n",
    "        return model\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of filtering models by symbol\n",
    "models_aapl = TrainedModels.query.filter_by(symbol='AAPL').all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fetch Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(symbol, start_date, end_date):\n",
    "    data = None\n",
    "    try:\n",
    "        # Fetch historical price data\n",
    "        df = yf.download(symbol, start=start_date, end=end_date)\n",
    "\n",
    "        # Drop 'Adj Close' column if present\n",
    "        if 'Adj Close' in df.columns:\n",
    "            df.drop(columns=['Adj Close'], inplace=True)\n",
    "\n",
    "        # Store the dataframe with technical indicators\n",
    "        data = df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/process_data', methods=['POST'])\n",
    "def process_data():\n",
    "    # Data from Firebase\n",
    "    data = request.json\n",
    "    symbol = data['symbol']\n",
    "    data_length = data['data_length']\n",
    "    forecast_days = data['forecast_days']\n",
    "    model_type = data['model_type']\n",
    "\n",
    "    #Set start and end date\n",
    "    now = datetime.now()\n",
    "    start_date =  (now - timedelta(days = data_length)).strftime(\"%Y-%m-%d\")\n",
    "    end_date = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    #Check if model already exists\n",
    "    model = find_model(model_type= model_type, start_date= start_date, end_date= end_date, symbol= symbol)\n",
    "    forecast_prices = None\n",
    "    if model:  # if the model is already trained avoid re-training it.\n",
    "        trained_model = model\n",
    "        # Forecast\n",
    "        forecast_prices = trained_model.forecast(forecast_days = forecast_days)\n",
    "    else:\n",
    "        data = fetch_data(symbol, start_date, end_date)\n",
    "        model = create_model(model_type, data)\n",
    "        trained_model = model.train()\n",
    "        # Create a new instance of Model\n",
    "        new_model = TrainedModels(symbol= symbol, model_type= model_type, start_date= start_date, end_date= end_date)\n",
    "        # Save the trained model to the database\n",
    "        new_model.save_trained_model(trained_model)\n",
    "        # Commit the changes\n",
    "        db.session.add(new_model)\n",
    "        db.session.commit()\n",
    "        # Forecast\n",
    "        forecast_prices = trained_model.forecast(forecast_days = forecast_days)\n",
    "\n",
    "    # Example: Data to send to Firebase\n",
    "    processed_data = {\n",
    "        'forecast_prices': forecast_prices,\n",
    "        'processed': True\n",
    "    }\n",
    "\n",
    "    return jsonify(processed_data), 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
