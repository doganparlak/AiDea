{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages & Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, render_template, redirect, url_for, session, flash\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "from werkzeug.security import check_password_hash, generate_password_hash\n",
    "from abc import ABC, abstractmethod\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from arch import arch_model\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from itertools import product\n",
    "import pickle\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import base64\n",
    "import io\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.patches as mpatches\n",
    "import optuna\n",
    "# Suppress all warnings from statsmodels\n",
    "warnings.filterwarnings(\"ignore\", category=Warning, module='statsmodels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "# Configure the SQLAlchemy part of the app instance\n",
    "app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///aidea.db'\n",
    "app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n",
    "app.secret_key = 'Doggan98-ddd'\n",
    "# Create the SQLAlchemy db instance\n",
    "db = SQLAlchemy(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Model Class Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(ABC):\n",
    "    def __init__(self, data, open, high, low, volume, symbol_name):\n",
    "        self.data = data\n",
    "        self.open = open\n",
    "        self.volume = volume\n",
    "        self.high = high\n",
    "        self.low = low\n",
    "        self.symbol_name = symbol_name\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Abstract method to train the model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forecast(self, forecast_days):\n",
    "        \"\"\"\n",
    "        Abstract method to make predictions using the trained model.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 AR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AR_model(Model):\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        data = data.dropna()\n",
    "        if not isinstance(data.index, pd.DatetimeIndex):\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "        if not data.index.freq:\n",
    "            # Attempt to infer the frequency\n",
    "            inferred_freq = pd.infer_freq(data.index)\n",
    "            if inferred_freq:\n",
    "                data.index.freq = inferred_freq\n",
    "            else:\n",
    "                # Handle the case where frequency cannot be inferred\n",
    "                # For example, you might decide to use a default frequency or handle this as an exception\n",
    "                print(\"Unable to infer frequency for the datetime index.\")\n",
    "\n",
    "        data.index.length = len(data)\n",
    "        return data\n",
    "    \n",
    "    def __init__(self, data, symbol_name):\n",
    "        data = self.prepare_data(data)\n",
    "        super().__init__(data = data['Close'], open = data['Open'], high = data['High'], low = data['Low'], volume = data['Volume'], symbol_name = symbol_name)\n",
    "        self.trained_model = None\n",
    "        self.model_type = 'Autoregressive'\n",
    "        self.stationary = False\n",
    "        self.show_backtest = True\n",
    "\n",
    "    def check_stationarity(self, series, alpha=0.05):\n",
    "        series = series.dropna()\n",
    "        result = adfuller(series)\n",
    "        p_value = result[1]\n",
    "        self.stationary = p_value < alpha\n",
    "        return self.stationary \n",
    "\n",
    "    def log_transform(self, series):\n",
    "        return np.log(series).dropna()\n",
    "    \n",
    "    def train(self):\n",
    "            # Check stationarity and apply log transformation if needed\n",
    "            data = self.data\n",
    "            if not self.check_stationarity(self.data):\n",
    "                print(\"Series is not stationary. Applying log transformation...\")\n",
    "                data = self.log_transform(self.data)\n",
    "                    \n",
    "            # Define parameter grid for tuning\n",
    "            trends = ['n', 'c', 't', 'ct']\n",
    "            min_lag = 1\n",
    "            max_lag = int(np.sqrt(len(data))) if len(data) >= 20 else len(data) // 2  # Ensure a practical upper bound for small datasets\n",
    "            lags_range = range(min_lag, max_lag + 1) \n",
    "\n",
    "            best_mse = float('inf')\n",
    "            best_params = 'n', 1\n",
    "            self.last_val_predictions = None  # To store the last validation split predictions\n",
    "            self.last_val_index = None  # To store the index of the last validation split\n",
    "\n",
    "            # Perform grid search with cross-validation on the training set\n",
    "            # Choose the best params based on MSE score\n",
    "            n_splits = 2\n",
    "            tscv = TimeSeriesSplit(n_splits=n_splits)  # Time series cross-validation\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            for trend, lags in product(trends, lags_range):\n",
    "                mse_sum = 0\n",
    "                val_predictions = None\n",
    "                val_index = None\n",
    "                for train_index, val_index in tscv.split(data):\n",
    "                    train_split, val_split = data.iloc[train_index], data.iloc[val_index]\n",
    "                    try:\n",
    "                        model = AutoReg(train_split.values, lags=lags, trend=trend).fit()\n",
    "                        predictions = model.predict(start=len(train_split), end=len(train_split) + len(val_split) - 1)\n",
    "\n",
    "                        mse = mean_squared_error(val_split, predictions)\n",
    "                        mse_sum += mse\n",
    "                        # Store the predictions and index of the last validation split\n",
    "                        if val_index[0] == len(data) - len(val_split):\n",
    "                           val_predictions = predictions\n",
    "                           val_index = val_index\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                # Average mse score across folds\n",
    "                avg_mse = mse_sum / n_splits\n",
    "                \n",
    "                # Update best parameters if better mse found\n",
    "                if avg_mse < best_mse:\n",
    "                    best_mse = avg_mse\n",
    "                    best_params = (trend, lags)\n",
    "                    self.last_val_predictions = val_predictions\n",
    "                    self.last_val_index = val_index\n",
    "\n",
    "            best_trend, best_lags = best_params\n",
    "        \n",
    "            print(f\"Best MSE score: {best_mse:.4f}\")\n",
    "            print(f\"Best parameters: trend={best_trend}, lags={best_lags}\")\n",
    "        \n",
    "            # Fit the best model on the entire dataset \n",
    "            try:\n",
    "                self.trained_model = AutoReg(data, lags=best_lags, trend=best_trend).fit()\n",
    "                print(f'Model training successful')\n",
    "            except Exception as e:\n",
    "                print(f'Model training failed with the error message: {e}')\n",
    "            \n",
    "    def forecast(self, forecast_days):\n",
    "        #Forecast next forecast_period days\n",
    "        start = len(self.data)\n",
    "        end = start + forecast_days - 1\n",
    "        forecast_prices = self.trained_model.predict(start=start, end=end)\n",
    "       # Reverse log transformation if applied\n",
    "        if not self.stationary:\n",
    "            forecast_prices = np.exp(forecast_prices)\n",
    "            if self.last_val_predictions is not None and self.last_val_index is not None:\n",
    "               self.last_val_predictions = np.exp(self.last_val_predictions)\n",
    "\n",
    "        # Plot the data\n",
    "        # Create date range for forecasted data\n",
    "        forecast_dates = pd.date_range(start=self.data.index[-1] + pd.Timedelta(days=1), periods=forecast_days, freq='D')\n",
    "        # Create figure and axis\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True, figsize=(16, 8), gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "        # Create candlestick data\n",
    "        candlestick_data = pd.DataFrame({\n",
    "            'Date': self.data.index,\n",
    "            'Open': self.open,\n",
    "            'Close': self.data,\n",
    "            'High': self.high,\n",
    "            'Low': self.low\n",
    "        })\n",
    "        # Plot the candlestick data with decreased transparency\n",
    "        for idx, row in candlestick_data.iterrows():\n",
    "            date_num = mdates.date2num(row['Date'])\n",
    "            if row['Close'] >= row['Open']:\n",
    "                color = 'green'\n",
    "                lower = row['Open']\n",
    "                height = row['Close'] - row['Open']\n",
    "            else:\n",
    "                color = 'red'\n",
    "                lower = row['Close']\n",
    "                height = row['Open'] - row['Close']\n",
    "            \n",
    "            # Draw high and low lines (wicks) outside the rectangle\n",
    "            ax1.vlines(date_num, row['Low'], lower, color=color, alpha=0.5, linewidth=0.5)\n",
    "            ax1.vlines(date_num, lower + height, row['High'], color=color, alpha=0.5, linewidth=0.5)\n",
    "            \n",
    "            # Draw the rectangle (candlestick body)\n",
    "            ax1.add_patch(mpatches.Rectangle((date_num - 0.5, lower), 1, height, edgecolor=color, facecolor=color, alpha=1, linewidth=1))\n",
    "        # Plot the price data\n",
    "        ax1.plot(self.data.index, self.data, label='Historical Data', color='gray', linewidth=1, alpha=0.6)\n",
    "        ax1.plot(forecast_dates, forecast_prices, label='Forecasted Prices', color='black', linewidth=1.5, linestyle = '-')\n",
    "        ax1.set_title(f'Model: {self.model_type} \\n Symbol: {self.symbol_name}', weight = 'bold', fontsize = 16)\n",
    "        ax1.set_ylabel('Price', weight = 'bold', fontsize = 15)\n",
    "       \n",
    "        ax1.grid(True, alpha = 0.3)\n",
    "        # Plot the last validation split predictions if available\n",
    "        if self.show_backtest:\n",
    "            if self.last_val_predictions is not None and self.last_val_index is not None:\n",
    "                ax1.plot(self.data.index[self.last_val_index], self.last_val_predictions, label='Backtest Predictions', color='dimgray', linewidth=1.5, linestyle='-')\n",
    "        ax1.legend(loc='upper left')\n",
    "        # Plot the volume data\n",
    "        volume_colors = np.where(self.data.diff() >= 0, 'green', 'red')\n",
    "        ax2.bar(self.data.index, self.volume, color=volume_colors, alpha=0.6)\n",
    "        ax2.set_ylabel('Volume', weight = 'bold', fontsize = 15)\n",
    "        ax2.set_xlabel('Time', weight = 'bold', fontsize = 15)\n",
    "        ax2.grid(True, alpha = 0.3)\n",
    "        \n",
    "        # Save plot to a bytes buffer\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        plot_data = base64.b64encode(buf.read()).decode('utf-8')\n",
    "        buf.close()\n",
    "        #plt.show()\n",
    "        plt.close(fig)  # Close the plot to free up resources\n",
    "\n",
    "        return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'AAPL'\n",
    "data_length = 1080\n",
    "forecast_days = 60\n",
    "model_type = 'AR'\n",
    "#Set start and end date\n",
    "now = datetime.now()\n",
    "start_date =  (now - timedelta(days = data_length)).strftime(\"%Y-%m-%d\")\n",
    "end_date = now.strftime(\"%Y-%m-%d\")\n",
    "data = fetch_data(symbol, start_date, end_date)\n",
    "model = AR_model(data, symbol)\n",
    "model.train()\n",
    "model.forecast(forecast_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARIMA_model(Model):\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        data = data.dropna()\n",
    "        if not isinstance(data.index, pd.DatetimeIndex):\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "        if not data.index.freq:\n",
    "            # Attempt to infer the frequency\n",
    "            inferred_freq = pd.infer_freq(data.index)\n",
    "            if inferred_freq:\n",
    "                data.index.freq = inferred_freq\n",
    "            else:\n",
    "                # Handle the case where frequency cannot be inferred\n",
    "                # For example, you might decide to use a default frequency or handle this as an exception\n",
    "                print(\"Unable to infer frequency for the datetime index.\")\n",
    "\n",
    "        data.index.length = len(data)\n",
    "        return data\n",
    "    \n",
    "    def __init__(self, data, symbol_name):\n",
    "        data = self.prepare_data(data)\n",
    "        super().__init__(data = data['Close'], open = data['Open'], high = data['High'], low = data['Low'], volume = data['Volume'], symbol_name = symbol_name)\n",
    "        self.trained_model = None\n",
    "        self.model_type = 'Autoregressive Integrated Moving Average'\n",
    "        self.stationary = False\n",
    "        self.show_backtest = True\n",
    "    def check_stationarity(self, series, alpha=0.05):\n",
    "        series = series.dropna()\n",
    "        result = adfuller(series)\n",
    "        p_value = result[1]\n",
    "        self.stationary = p_value < alpha\n",
    "        return self.stationary \n",
    "\n",
    "    def log_transform(self, series):\n",
    "        return np.log(series).dropna()\n",
    "    \n",
    "    def train(self):\n",
    "            # Check stationarity and apply log transformation if needed\n",
    "            data = self.data\n",
    "            if not self.check_stationarity(self.data):\n",
    "                print(\"Series is not stationary. Applying log transformation...\")\n",
    "                data = self.log_transform(self.data)\n",
    "                    \n",
    "            # Define parameter grid for tuning\n",
    "            p_values = range(0, 3)\n",
    "            d_values = range(0, 2)\n",
    "            q_values = range(0, 3)\n",
    "            trends = ['c', 't', 'ct', [0, 0, 1, 0], [0, 0, 0, 1]]\n",
    "            \n",
    "            best_mse = float('inf') \n",
    "            best_params = 'c', 0, 0, 0\n",
    "            self.last_val_predictions = None  # To store the last validation split predictions\n",
    "            self.last_val_index = None  # To store the index of the last validation split\n",
    "\n",
    "            # Perform grid search with cross-validation on the training set\n",
    "            # Choose the best params based on MSE score\n",
    "            n_splits = 2\n",
    "            tscv = TimeSeriesSplit(n_splits=n_splits)  # Time series cross-validation\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            for d in d_values:\n",
    "                if d == 1:\n",
    "                    trends = ['t', [0,0,1,0], [0, 0, 0, 1]]\n",
    "                elif d == 2:\n",
    "                    trends = [[0,0,1,0], [0,0,0,1]]\n",
    "                elif d == 0:\n",
    "                    trends = ['c', 't', 'ct', [0, 0, 1, 0], [0, 0, 0, 1]]\n",
    "                for trend in trends:\n",
    "                    for p in p_values:   \n",
    "                        for q in q_values:\n",
    "                            mse_sum = 0\n",
    "                            val_predictions = None\n",
    "                            val_index = None\n",
    "                            for train_index, val_index in tscv.split(data):\n",
    "                                train_split, val_split = data.iloc[train_index], data.iloc[val_index]\n",
    "                                try:\n",
    "                                    model = ARIMA(train_split, order=(p,d,q), trend = trend).fit()\n",
    "                                    predictions = model.predict(start=len(train_split), end=len(train_split) + len(val_split) - 1)\n",
    "                                    mse = mean_squared_error(val_split, predictions)\n",
    "                                    mse_sum += mse\n",
    "                                    # Store the predictions and index of the last validation split\n",
    "                                    if val_index[0] == len(data) - len(val_split):\n",
    "                                        val_predictions = predictions\n",
    "                                        val_index = val_index\n",
    "                                except Exception as e:\n",
    "                                    continue\n",
    "\n",
    "                            avg_mse = mse_sum / n_splits\n",
    "                            if avg_mse < best_mse:\n",
    "                                best_mse = avg_mse\n",
    "                                best_params = (trend, p, d, q)\n",
    "                                self.last_val_predictions = val_predictions\n",
    "                                self.last_val_index = val_index\n",
    "\n",
    "            best_trend, best_p, best_d, best_q = best_params\n",
    "            print(f\"Best MSE score: {best_mse:.4f}\")\n",
    "            print(f\"Best parameters: trend={best_trend}, p={best_p}, d={best_d}, q={best_q}\")\n",
    "        \n",
    "            # Fit the best model on the entire dataset \n",
    "            try:\n",
    "                self.trained_model = ARIMA(data, order=(best_p,best_d,best_q), trend=best_trend).fit()\n",
    "                print(f'Model training successful')\n",
    "            except Exception as e:\n",
    "                print(f'Model training failed with the error message: {e}')\n",
    "            \n",
    "    def forecast(self, forecast_days):\n",
    "        #Forecast next forecast_period days\n",
    "        start = len(self.data)\n",
    "        end = start + forecast_days - 1\n",
    "        forecast_prices = self.trained_model.predict(start=start, end=end)\n",
    "       # Reverse log transformation if applied\n",
    "        if not self.stationary:\n",
    "            forecast_prices = np.exp(forecast_prices)\n",
    "            if self.last_val_predictions is not None and self.last_val_index is not None:\n",
    "                self.last_val_predictions = np.exp(self.last_val_predictions)\n",
    "\n",
    "        # Plot the data\n",
    "        # Create date range for forecasted data\n",
    "        forecast_dates = pd.date_range(start=self.data.index[-1] + pd.Timedelta(days=1), periods=forecast_days, freq='D')\n",
    "        # Create figure and axis\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True, figsize=(16, 8), gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "        # Create candlestick data\n",
    "        candlestick_data = pd.DataFrame({\n",
    "            'Date': self.data.index,\n",
    "            'Open': self.open,\n",
    "            'Close': self.data,\n",
    "            'High': self.high,\n",
    "            'Low': self.low\n",
    "        })\n",
    "        # Plot the candlestick data with decreased transparency\n",
    "        for idx, row in candlestick_data.iterrows():\n",
    "            date_num = mdates.date2num(row['Date'])\n",
    "            if row['Close'] >= row['Open']:\n",
    "                color = 'green'\n",
    "                lower = row['Open']\n",
    "                height = row['Close'] - row['Open']\n",
    "            else:\n",
    "                color = 'red'\n",
    "                lower = row['Close']\n",
    "                height = row['Open'] - row['Close']\n",
    "            \n",
    "            # Draw high and low lines (wicks) outside the rectangle\n",
    "            ax1.vlines(date_num, row['Low'], lower, color=color, alpha=0.5, linewidth=0.5)\n",
    "            ax1.vlines(date_num, lower + height, row['High'], color=color, alpha=0.5, linewidth=0.5)\n",
    "            \n",
    "            # Draw the rectangle (candlestick body)\n",
    "            ax1.add_patch(mpatches.Rectangle((date_num - 0.5, lower), 1, height, edgecolor=color, facecolor=color, alpha=1, linewidth=1))\n",
    "\n",
    "        # Plot the price data\n",
    "        ax1.plot(self.data.index, self.data, label='Historical Data', color='gray', linewidth=1, alpha=0.6)\n",
    "        ax1.plot(forecast_dates, forecast_prices, label='Forecasted Prices', color='black', linewidth=1.5, linestyle = '-')\n",
    "        ax1.set_title(f'Model: {self.model_type} \\n Symbol: {self.symbol_name}', weight = 'bold', fontsize = 16)\n",
    "        ax1.set_ylabel('Price', weight = 'bold', fontsize = 15)\n",
    "        ax1.grid(True, alpha = 0.3)\n",
    "\n",
    "        # Plot the last validation split predictions if available\n",
    "        if self.show_backtest:\n",
    "            if self.last_val_predictions is not None and self.last_val_index is not None:\n",
    "                ax1.plot(self.data.index[self.last_val_index], self.last_val_predictions, label='Backtest Predictions', color='dimgray', linewidth=1.5, linestyle='-')\n",
    "\n",
    "        ax1.legend(loc='upper left')\n",
    "        # Plot the volume data\n",
    "        volume_colors = np.where(self.data.diff() >= 0, 'green', 'red')\n",
    "        ax2.bar(self.data.index, self.volume, color=volume_colors, alpha=0.6)\n",
    "        ax2.set_ylabel('Volume', weight = 'bold', fontsize = 15)\n",
    "        ax2.set_xlabel('Time', weight = 'bold', fontsize = 15)\n",
    "        ax2.grid(True, alpha = 0.3)\n",
    "        \n",
    "        # Save plot to a bytes buffer\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        plot_data = base64.b64encode(buf.read()).decode('utf-8')\n",
    "        buf.close()\n",
    "        plt.show()\n",
    "        #plt.close(fig)  # Close the plot to free up resources\n",
    "\n",
    "        return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'AMZN'\n",
    "data_length = 1080\n",
    "forecast_days = 90\n",
    "#Set start and end date\n",
    "now = datetime.now()\n",
    "start_date =  (now - timedelta(days = data_length)).strftime(\"%Y-%m-%d\")\n",
    "end_date = now.strftime(\"%Y-%m-%d\")\n",
    "data = fetch_data(symbol, start_date, end_date)\n",
    "model = ARIMA_model(data, symbol)\n",
    "model.train()\n",
    "model.forecast(forecast_days)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 SARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARIMA_model(Model):\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        data = data.dropna()\n",
    "        if not isinstance(data.index, pd.DatetimeIndex):\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "        if not data.index.freq:\n",
    "            # Attempt to infer the frequency\n",
    "            inferred_freq = pd.infer_freq(data.index)\n",
    "            if inferred_freq:\n",
    "                data.index.freq = inferred_freq\n",
    "            else:\n",
    "                # Handle the case where frequency cannot be inferred\n",
    "                # For example, you might decide to use a default frequency or handle this as an exception\n",
    "                print(\"Unable to infer frequency for the datetime index.\")\n",
    "\n",
    "        data.index.length = len(data)\n",
    "        return data\n",
    "    \n",
    "    def __init__(self, data, symbol_name):\n",
    "        data = self.prepare_data(data)\n",
    "        super().__init__(data = data['Close'], open = data['Open'], high = data['High'], low = data['Low'], volume = data['Volume'], symbol_name = symbol_name)\n",
    "        self.trained_model = None\n",
    "        self.model_type = 'Seasonal Autoregressive Integrated Moving Average'\n",
    "        self.stationary = False\n",
    "        self.show_backtest = True\n",
    "\n",
    "    def check_stationarity(self, series, alpha=0.05):\n",
    "        series = series.dropna()\n",
    "        result = adfuller(series)\n",
    "        p_value = result[1]\n",
    "        self.stationary = p_value < alpha\n",
    "        return self.stationary \n",
    "\n",
    "    def log_transform(self, series):\n",
    "        return np.log(series).dropna()\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        p = trial.suggest_int('p', 0, 3)\n",
    "        d = trial.suggest_int('d', 0, 2)\n",
    "        q = trial.suggest_int('q', 0, 3)\n",
    "        P = trial.suggest_int('P', 0, 2)\n",
    "        D = trial.suggest_int('D', 0, 1)\n",
    "        Q = trial.suggest_int('Q', 0, 2)\n",
    "        s = trial.suggest_categorical('s', [7, 12, 30, 52])  # Example seasonal periods\n",
    "        trend = trial.suggest_categorical('trend', ['c', 't', 'ct'])\n",
    "\n",
    "        mse_sum = 0\n",
    "        n_splits = 2\n",
    "        best_val_predictions = None\n",
    "        best_val_index = None\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        # Retrieve the current best_mse from user attributes\n",
    "        best_mse = trial.user_attrs.get('best_mse', float('inf'))\n",
    "\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        for train_index, val_index in tscv.split(self.data):\n",
    "            train_split, val_split = self.data.iloc[train_index], self.data.iloc[val_index]\n",
    "            try:\n",
    "                model = SARIMAX(train_split, order=(p, d, q), seasonal_order=(P, D, Q, s), trend=trend).fit(disp = False)\n",
    "                predictions = model.predict(start=len(train_split), end=len(train_split) + len(val_split) - 1)\n",
    "                mse = mean_squared_error(val_split, predictions)\n",
    "                mse_sum += mse\n",
    "                # Store the predictions and index for the last validation split\n",
    "                if len(val_index) > 0 and val_index[0] == len(self.data) - len(val_split):\n",
    "                    best_val_predictions = predictions\n",
    "                    best_val_index = val_index\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                return float('inf')  # Return a very low value if an error occurs\n",
    "\n",
    "        avg_mse = mse_sum / n_splits\n",
    "        if avg_mse<best_mse:\n",
    "            # Store the best predictions and index within the trial object for later retrieval\n",
    "            trial.set_user_attr(\"best_mse\", avg_mse)\n",
    "            trial.set_user_attr(\"best_val_predictions\", best_val_predictions)\n",
    "            trial.set_user_attr(\"best_val_index\", best_val_index)\n",
    "\n",
    "        return avg_mse\n",
    "    \n",
    "    def train(self):\n",
    "            # Check stationarity and apply log transformation if needed\n",
    "            if not self.check_stationarity(self.data):\n",
    "                print(\"Series is not stationary. Applying log transformation...\")\n",
    "                self.data = self.log_transform(self.data)\n",
    "                    \n",
    "             # Create an Optuna study\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            # Define an initial best_mse as infinity\n",
    "            initial_best_mse = float('inf')\n",
    "            # Define the objective function with an initial best_mse\n",
    "            def objective_with_initial_best_mse(trial):\n",
    "                trial.set_user_attr(\"best_mse\", initial_best_mse)\n",
    "                return self.objective(trial)\n",
    "\n",
    "            study.optimize(objective_with_initial_best_mse, n_trials=30)  # Number of trials can be adjusted\n",
    "\n",
    "\n",
    "            best_params = study.best_params\n",
    "            best_mse = study.best_value  # Access custom attributes returned from objective function\n",
    "\n",
    "            # Retrieve the best trial\n",
    "            best_trial = study.best_trial\n",
    "\n",
    "            # Store the best validation predictions and index\n",
    "            self.last_val_predictions = best_trial.user_attrs[\"best_val_predictions\"]\n",
    "            self.last_val_index = best_trial.user_attrs[\"best_val_index\"]\n",
    "\n",
    "            \n",
    "            print(f\"Best MSE score: {best_mse:.4f}\")\n",
    "            print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "            # Fit the best model on the entire dataset\n",
    "            try:\n",
    "                self.trained_model = SARIMAX(self.data, order=(best_params['p'], best_params['d'], best_params['q']),\n",
    "                                            seasonal_order=(best_params['P'], best_params['D'], best_params['Q'], best_params['s']),\n",
    "                                            trend=best_params['trend']).fit(disp = False)\n",
    "                print(f'Model training successful')\n",
    "            except Exception as e:\n",
    "                print(f'Model training failed with the error message: {e}')\n",
    "            \n",
    "    def forecast(self, forecast_days):\n",
    "        #Forecast next forecast_period days\n",
    "        start = len(self.data)\n",
    "        end = start + forecast_days - 1\n",
    "        forecast_prices = self.trained_model.predict(start=start, end=end)\n",
    "       # Reverse log transformation if applied\n",
    "        if not self.stationary:\n",
    "            self.data = np.exp(self.data)\n",
    "            forecast_prices = np.exp(forecast_prices)\n",
    "            if self.last_val_predictions is not None and self.last_val_index is not None:\n",
    "                self.last_val_predictions = np.exp(self.last_val_predictions)\n",
    "\n",
    "        # Plot the data\n",
    "        # Create date range for forecasted data\n",
    "        forecast_dates = pd.date_range(start=self.data.index[-1] + pd.Timedelta(days=1), periods=forecast_days, freq='D')\n",
    "        # Create figure and axis\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True, figsize=(16, 8), gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "        # Create candlestick data\n",
    "        candlestick_data = pd.DataFrame({\n",
    "            'Date': self.data.index,\n",
    "            'Open': self.open,\n",
    "            'Close': self.data,\n",
    "            'High': self.high,\n",
    "            'Low': self.low\n",
    "        })\n",
    "        # Plot the candlestick data with decreased transparency\n",
    "        for idx, row in candlestick_data.iterrows():\n",
    "            date_num = mdates.date2num(row['Date'])\n",
    "            if row['Close'] >= row['Open']:\n",
    "                color = 'green'\n",
    "                lower = row['Open']\n",
    "                height = row['Close'] - row['Open']\n",
    "            else:\n",
    "                color = 'red'\n",
    "                lower = row['Close']\n",
    "                height = row['Open'] - row['Close']\n",
    "            \n",
    "            # Draw high and low lines (wicks) outside the rectangle\n",
    "            ax1.vlines(date_num, row['Low'], lower, color=color, alpha=0.5, linewidth=0.5)\n",
    "            ax1.vlines(date_num, lower + height, row['High'], color=color, alpha=0.5, linewidth=0.5)\n",
    "            \n",
    "            # Draw the rectangle (candlestick body)\n",
    "            ax1.add_patch(mpatches.Rectangle((date_num - 0.5, lower), 1, height, edgecolor=color, facecolor=color, alpha=1, linewidth=1))\n",
    "        \n",
    "        # Plot the price data\n",
    "        ax1.plot(self.data.index, self.data, label='Historical Data', color='gray', linewidth=1, alpha=0.6)\n",
    "        ax1.plot(forecast_dates, forecast_prices, label='Forecasted Prices', color='black', linewidth=1.5, linestyle = '-')\n",
    "        ax1.set_title(f'Model: {self.model_type} \\n Symbol: {self.symbol_name}', weight = 'bold', fontsize = 16)\n",
    "        ax1.set_ylabel('Price', weight = 'bold', fontsize = 15)\n",
    "        ax1.grid(True, alpha = 0.3)\n",
    "\n",
    "        # Plot the last validation split predictions if available\n",
    "        if self.show_backtest:\n",
    "            if self.last_val_predictions is not None and self.last_val_index is not None:\n",
    "                ax1.plot(self.data.index[self.last_val_index], self.last_val_predictions, label='Backtest Predictions', color='dimgray', linewidth=1.5, linestyle='-')\n",
    "\n",
    "        ax1.legend(loc='upper left')\n",
    "        # Plot the volume data\n",
    "        volume_colors = np.where(self.data.diff() >= 0, 'green', 'red')\n",
    "        ax2.bar(self.data.index, self.volume, color=volume_colors, alpha=0.6)\n",
    "        ax2.set_ylabel('Volume', weight = 'bold', fontsize = 15)\n",
    "        ax2.set_xlabel('Time', weight = 'bold', fontsize = 15)\n",
    "        ax2.grid(True, alpha = 0.3)\n",
    "        \n",
    "        # Save plot to a bytes buffer\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        plot_data = base64.b64encode(buf.read()).decode('utf-8')\n",
    "        buf.close()\n",
    "        plt.show()\n",
    "        #plt.close(fig)  # Close the plot to free up resources\n",
    "\n",
    "        return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-21 16:18:44,551] A new study created in memory with name: no-name-62b4c2fe-39c2-4e35-982f-f06e675b02be\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unable to infer frequency for the datetime index.\n",
      "Series is not stationary. Applying log transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-21 16:18:46,312] Trial 0 finished with value: 0.7764241866505251 and parameters: {'p': 1, 'd': 1, 'q': 0, 'P': 0, 'D': 1, 'Q': 2, 's': 12, 'trend': 't'}. Best is trial 0 with value: 0.7764241866505251.\n",
      "[I 2024-08-21 16:19:11,574] Trial 1 finished with value: 5844.889991513328 and parameters: {'p': 2, 'd': 2, 'q': 2, 'P': 0, 'D': 1, 'Q': 1, 's': 52, 'trend': 'ct'}. Best is trial 0 with value: 0.7764241866505251.\n",
      "[I 2024-08-21 16:19:12,414] Trial 2 finished with value: 0.02525105603931375 and parameters: {'p': 1, 'd': 1, 'q': 3, 'P': 1, 'D': 0, 'Q': 0, 's': 12, 'trend': 'c'}. Best is trial 2 with value: 0.02525105603931375.\n",
      "[I 2024-08-21 16:19:16,290] Trial 3 finished with value: 0.011554595084724048 and parameters: {'p': 2, 'd': 0, 'q': 3, 'P': 0, 'D': 1, 'Q': 2, 's': 12, 'trend': 'c'}. Best is trial 3 with value: 0.011554595084724048.\n",
      "[I 2024-08-21 16:19:17,848] Trial 4 finished with value: 0.023245555640270896 and parameters: {'p': 3, 'd': 0, 'q': 3, 'P': 0, 'D': 0, 'Q': 1, 's': 12, 'trend': 'ct'}. Best is trial 3 with value: 0.011554595084724048.\n",
      "[I 2024-08-21 16:20:08,994] Trial 5 finished with value: 0.011629487397440769 and parameters: {'p': 3, 'd': 0, 'q': 3, 'P': 2, 'D': 0, 'Q': 2, 's': 52, 'trend': 'c'}. Best is trial 3 with value: 0.011554595084724048.\n",
      "[I 2024-08-21 16:20:11,055] Trial 6 finished with value: 0.013801166806335082 and parameters: {'p': 2, 'd': 0, 'q': 2, 'P': 0, 'D': 0, 'Q': 2, 's': 12, 'trend': 'c'}. Best is trial 3 with value: 0.011554595084724048.\n",
      "[I 2024-08-21 16:20:15,421] Trial 7 finished with value: 7.852781982372916 and parameters: {'p': 0, 'd': 2, 'q': 0, 'P': 1, 'D': 1, 'Q': 0, 's': 30, 'trend': 'c'}. Best is trial 3 with value: 0.011554595084724048.\n",
      "[I 2024-08-21 16:20:16,913] Trial 8 finished with value: 0.043772562874676124 and parameters: {'p': 2, 'd': 0, 'q': 3, 'P': 1, 'D': 1, 'Q': 0, 's': 12, 'trend': 't'}. Best is trial 3 with value: 0.011554595084724048.\n",
      "[I 2024-08-21 16:20:33,594] Trial 9 finished with value: 0.42129236409269943 and parameters: {'p': 3, 'd': 2, 'q': 3, 'P': 1, 'D': 0, 'Q': 2, 's': 30, 'trend': 'c'}. Best is trial 3 with value: 0.011554595084724048.\n",
      "[I 2024-08-21 16:20:34,454] Trial 10 finished with value: 2.826733426635207 and parameters: {'p': 0, 'd': 1, 'q': 1, 'P': 2, 'D': 1, 'Q': 1, 's': 7, 'trend': 't'}. Best is trial 3 with value: 0.011554595084724048.\n",
      "[I 2024-08-21 16:21:15,537] Trial 11 finished with value: 0.013071877886294736 and parameters: {'p': 3, 'd': 0, 'q': 2, 'P': 2, 'D': 0, 'Q': 2, 's': 52, 'trend': 'c'}. Best is trial 3 with value: 0.011554595084724048.\n",
      "[I 2024-08-21 16:22:34,177] Trial 12 finished with value: 0.03377250715101695 and parameters: {'p': 3, 'd': 0, 'q': 1, 'P': 2, 'D': 1, 'Q': 2, 's': 52, 'trend': 'c'}. Best is trial 3 with value: 0.011554595084724048.\n",
      "[I 2024-08-21 16:22:37,621] Trial 13 finished with value: 0.012731287047947702 and parameters: {'p': 2, 'd': 0, 'q': 3, 'P': 2, 'D': 0, 'Q': 2, 's': 7, 'trend': 'c'}. Best is trial 3 with value: 0.011554595084724048.\n",
      "[I 2024-08-21 16:23:02,664] Trial 14 finished with value: 0.08654144750289863 and parameters: {'p': 1, 'd': 1, 'q': 2, 'P': 1, 'D': 1, 'Q': 1, 's': 52, 'trend': 'c'}. Best is trial 3 with value: 0.011554595084724048.\n",
      "[W 2024-08-21 16:23:58,277] Trial 15 failed with parameters: {'p': 3, 'd': 0, 'q': 3, 'P': 2, 'D': 0, 'Q': 2, 's': 52, 'trend': 'ct'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-88-3d9cacfb050f>\", line 94, in objective_with_initial_best_mse\n",
      "    return self.objective(trial)\n",
      "  File \"<ipython-input-88-3d9cacfb050f>\", line 60, in objective\n",
      "    model = SARIMAX(train_split, order=(p, d, q), seasonal_order=(P, D, Q, s), trend=trend).fit()\n",
      "  File \"/usr/local/lib/python3.8/site-packages/statsmodels/tsa/statespace/mlemodel.py\", line 704, in fit\n",
      "    mlefit = super(MLEModel, self).fit(start_params, method=method,\n",
      "  File \"/usr/local/lib/python3.8/site-packages/statsmodels/base/model.py\", line 566, in fit\n",
      "    xopt, retvals, optim_settings = optimizer._fit(f, score, start_params,\n",
      "  File \"/usr/local/lib/python3.8/site-packages/statsmodels/base/optimizer.py\", line 242, in _fit\n",
      "    xopt, retvals = func(objective, gradient, start_params, fargs, kwargs,\n",
      "  File \"/usr/local/lib/python3.8/site-packages/statsmodels/base/optimizer.py\", line 659, in _fit_lbfgs\n",
      "    retvals = optimize.fmin_l_bfgs_b(func, start_params, maxiter=maxiter,\n",
      "  File \"/usr/local/lib/python3.8/site-packages/scipy/optimize/lbfgsb.py\", line 197, in fmin_l_bfgs_b\n",
      "    res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n",
      "  File \"/usr/local/lib/python3.8/site-packages/scipy/optimize/lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/usr/local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 201, in fun_and_grad\n",
      "    self._update_grad()\n",
      "  File \"/usr/local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 171, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/usr/local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 91, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/usr/local/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 426, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/usr/local/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 497, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/usr/local/lib/python3.8/site-packages/scipy/optimize/_numdiff.py\", line 377, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/usr/local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 70, in fun_wrapped\n",
      "    return fun(x, *args)\n",
      "  File \"/usr/local/lib/python3.8/site-packages/statsmodels/base/model.py\", line 534, in f\n",
      "    return -self.loglike(params, *args) / nobs\n",
      "  File \"/usr/local/lib/python3.8/site-packages/statsmodels/tsa/statespace/mlemodel.py\", line 939, in loglike\n",
      "    loglike = self.ssm.loglike(complex_step=complex_step, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/site-packages/statsmodels/tsa/statespace/kalman_filter.py\", line 1001, in loglike\n",
      "    kfilter = self._filter(**kwargs)\n",
      "  File \"/usr/local/lib/python3.8/site-packages/statsmodels/tsa/statespace/kalman_filter.py\", line 921, in _filter\n",
      "    self._initialize_state(prefix=prefix, complex_step=complex_step)\n",
      "  File \"/usr/local/lib/python3.8/site-packages/statsmodels/tsa/statespace/representation.py\", line 1058, in _initialize_state\n",
      "    self._statespaces[prefix].initialize(self.initialization,\n",
      "KeyboardInterrupt\n",
      "[W 2024-08-21 16:23:58,280] Trial 15 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m data \u001b[38;5;241m=\u001b[39m fetch_data(symbol, start_date, end_date)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m SARIMA_model(data, symbol)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mforecast(forecast_days)\n",
      "Input \u001b[0;32mIn [88]\u001b[0m, in \u001b[0;36mSARIMA_model.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m     trial\u001b[38;5;241m.\u001b[39mset_user_attr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_mse\u001b[39m\u001b[38;5;124m\"\u001b[39m, initial_best_mse)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective(trial)\n\u001b[0;32m---> 96\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_with_initial_best_mse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Number of trials can be adjusted\u001b[39;00m\n\u001b[1;32m     99\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[1;32m    100\u001b[0m best_mse \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_value  \u001b[38;5;66;03m# Access custom attributes returned from objective function\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[0;32mIn [88]\u001b[0m, in \u001b[0;36mSARIMA_model.train.<locals>.objective_with_initial_best_mse\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective_with_initial_best_mse\u001b[39m(trial):\n\u001b[1;32m     93\u001b[0m     trial\u001b[38;5;241m.\u001b[39mset_user_attr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_mse\u001b[39m\u001b[38;5;124m\"\u001b[39m, initial_best_mse)\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [88]\u001b[0m, in \u001b[0;36mSARIMA_model.objective\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m     58\u001b[0m train_split, val_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[train_index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[val_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mSARIMAX\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseasonal_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrend\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_split), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_split) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_split) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m     mse \u001b[38;5;241m=\u001b[39m mean_squared_error(val_split, predictions)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/statsmodels/tsa/statespace/mlemodel.py:704\u001b[0m, in \u001b[0;36mMLEModel.fit\u001b[0;34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m         flags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhessian_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m optim_hessian\n\u001b[1;32m    703\u001b[0m     fargs \u001b[38;5;241m=\u001b[39m (flags,)\n\u001b[0;32m--> 704\u001b[0m     mlefit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMLEModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mfargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mskip_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# Just return the fitted parameters if requested\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_params:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/statsmodels/base/model.py:566\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_t\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    565\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Optimizer()\n\u001b[0;32m--> 566\u001b[0m xopt, retvals, optim_settings \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mhessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[1;32m    576\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/statsmodels/base/optimizer.py:242\u001b[0m, in \u001b[0;36mOptimizer._fit\u001b[0;34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[0m\n\u001b[1;32m    239\u001b[0m     fit_funcs\u001b[38;5;241m.\u001b[39mupdate(extra_fit_funcs)\n\u001b[1;32m    241\u001b[0m func \u001b[38;5;241m=\u001b[39m fit_funcs[method]\n\u001b[0;32m--> 242\u001b[0m xopt, retvals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mhess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhessian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m optim_settings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: method, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_params\u001b[39m\u001b[38;5;124m'\u001b[39m: start_params,\n\u001b[1;32m    248\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: maxiter, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_output\u001b[39m\u001b[38;5;124m'\u001b[39m: full_output,\n\u001b[1;32m    249\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfargs\u001b[39m\u001b[38;5;124m'\u001b[39m: fargs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[1;32m    250\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretall\u001b[39m\u001b[38;5;124m'\u001b[39m: retall, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_fit_funcs\u001b[39m\u001b[38;5;124m\"\u001b[39m: extra_fit_funcs}\n\u001b[1;32m    251\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/statsmodels/base/optimizer.py:659\u001b[0m, in \u001b[0;36m_fit_lbfgs\u001b[0;34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m approx_grad:\n\u001b[1;32m    657\u001b[0m     func \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m--> 659\u001b[0m retvals \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin_l_bfgs_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[1;32m    665\u001b[0m     xopt, fopt, d \u001b[38;5;241m=\u001b[39m retvals\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/scipy/optimize/lbfgsb.py:197\u001b[0m, in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    185\u001b[0m     disp \u001b[38;5;241m=\u001b[39m iprint\n\u001b[1;32m    186\u001b[0m opts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp,\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m'\u001b[39m: iprint,\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxcor\u001b[39m\u001b[38;5;124m'\u001b[39m: m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m'\u001b[39m: maxls}\n\u001b[0;32m--> 197\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m d \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjac\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    200\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    201\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuncalls\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfev\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    202\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    203\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarnflag\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m    204\u001b[0m f \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/scipy/optimize/lbfgsb.py:360\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    354\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:201\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:171\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated:\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:91\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfinite_diff_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/scipy/optimize/_numdiff.py:426\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/scipy/optimize/_numdiff.py:497\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    495\u001b[0m     x \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n\u001b[1;32m    496\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    499\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/scipy/optimize/_numdiff.py:377\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_wrapped\u001b[39m(x):\n\u001b[0;32m--> 377\u001b[0m     f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    380\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:70\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_wrapped\u001b[39m(x):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/statsmodels/base/model.py:534\u001b[0m, in \u001b[0;36mLikelihoodModel.fit.<locals>.f\u001b[0;34m(params, *args)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(params, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m nobs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/statsmodels/tsa/statespace/mlemodel.py:939\u001b[0m, in \u001b[0;36mMLEModel.loglike\u001b[0;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m complex_step:\n\u001b[1;32m    937\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minversion_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m INVERT_UNIVARIATE \u001b[38;5;241m|\u001b[39m SOLVE_LU\n\u001b[0;32m--> 939\u001b[0m loglike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# Koopman, Shephard, and Doornik recommend maximizing the average\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# likelihood to avoid scale issues, but the averaging is done\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;66;03m# automatically in the base model `fit` method\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loglike\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/statsmodels/tsa/statespace/kalman_filter.py:1001\u001b[0m, in \u001b[0;36mKalmanFilter.loglike\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;124;03mCalculate the loglikelihood associated with the statespace model.\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;124;03m    The joint loglikelihood.\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    999\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconserve_memory\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1000\u001b[0m                   MEMORY_CONSERVE \u001b[38;5;241m^\u001b[39m MEMORY_NO_LIKELIHOOD)\n\u001b[0;32m-> 1001\u001b[0m kfilter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m loglikelihood_burn \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloglikelihood_burn\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1003\u001b[0m                                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloglikelihood_burn)\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconserve_memory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m&\u001b[39m MEMORY_NO_LIKELIHOOD):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/statsmodels/tsa/statespace/kalman_filter.py:921\u001b[0m, in \u001b[0;36mKalmanFilter._filter\u001b[0;34m(self, filter_method, inversion_method, stability_method, conserve_memory, filter_timing, tolerance, loglikelihood_burn, complex_step)\u001b[0m\n\u001b[1;32m    918\u001b[0m kfilter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kalman_filters[prefix]\n\u001b[1;32m    920\u001b[0m \u001b[38;5;66;03m# Initialize the state\u001b[39;00m\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Run the filter\u001b[39;00m\n\u001b[1;32m    924\u001b[0m kfilter()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/statsmodels/tsa/statespace/representation.py:1058\u001b[0m, in \u001b[0;36mRepresentation._initialize_state\u001b[0;34m(self, prefix, complex_step)\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialization\u001b[38;5;241m.\u001b[39minitialized:\n\u001b[1;32m   1057\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitialization is incomplete.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_statespaces\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatespace model not initialized.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "symbol = 'GSRAY.IS'\n",
    "data_length = 360\n",
    "forecast_days = 120\n",
    "#Set start and end date\n",
    "now = datetime.now()\n",
    "start_date =  (now - timedelta(days = data_length)).strftime(\"%Y-%m-%d\")\n",
    "end_date = now.strftime(\"%Y-%m-%d\")\n",
    "data = fetch_data(symbol, start_date, end_date)\n",
    "model = SARIMA_model(data, symbol)\n",
    "model.train()\n",
    "model.forecast(forecast_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Holt-Winters Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HWES_model(Model):\n",
    "    def prepare_data(self, data):\n",
    "        data = data.dropna()\n",
    "        if not isinstance(data.index, pd.DatetimeIndex):\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "        if not data.index.freq:\n",
    "            # Attempt to infer the frequency\n",
    "            inferred_freq = pd.infer_freq(data.index)\n",
    "            if inferred_freq:\n",
    "                data.index.freq = inferred_freq\n",
    "            else:\n",
    "                # Handle the case where frequency cannot be inferred\n",
    "                # For example, you might decide to use a default frequency or handle this as an exception\n",
    "                print(\"Unable to infer frequency for the datetime index.\")\n",
    "\n",
    "        data.index.length = len(data)\n",
    "        return data\n",
    "    \n",
    "    def __init__(self, data, symbol_name):\n",
    "        data = self.prepare_data(data)\n",
    "        super().__init__(data = data['Close'], open = data['Open'], high = data['High'], low = data['Low'], volume = data['Volume'], symbol_name = symbol_name)\n",
    "        self.trained_model = None\n",
    "        self.model_type = 'Holt-Winters Exponential Smoothing'\n",
    "        self.stationary = False\n",
    "        self.show_backtest = True\n",
    "\n",
    "    def check_stationarity(self, series, alpha=0.05):\n",
    "        series = series.dropna()\n",
    "        result = adfuller(series)\n",
    "        p_value = result[1]\n",
    "        self.stationary = p_value < alpha\n",
    "        return self.stationary \n",
    "\n",
    "    def log_transform(self, series):\n",
    "        return np.log(series).dropna()\n",
    "    \n",
    "    def objective(self, trial):\n",
    "\n",
    "        # Define range of parameters\n",
    "        trend = trial.suggest_categorical('trend', ['add', 'mul', 'additive', 'multiplicative', None])\n",
    "        seasonal = trial.suggest_categorical('seasonal', ['add', 'mul', 'additive', 'multiplicative', None])\n",
    "        seasonal_periods = trial.suggest_categorical('seasonal_periods', [7, 12, 30, 52])  # Example seasonal periods\n",
    "        initialization_method = trial.suggest_categorical('initialization_method', [None, 'estimated', 'heuristic', 'legacy-heuristic'])\n",
    "        use_boxcox = trial.suggest_categorical('use_boxcox', [True, False])\n",
    "        mse_sum = 0\n",
    "        n_splits = 2\n",
    "        best_val_predictions = None\n",
    "        best_val_index = None\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        \n",
    "         # Retrieve the current best_mse from user attributes\n",
    "        best_mse = trial.user_attrs.get('best_mse', float('inf'))\n",
    "\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        for train_index, val_index in tscv.split(self.data):\n",
    "            train_split, val_split = self.data.iloc[train_index], self.data.iloc[val_index]\n",
    "            try:\n",
    "                model = ExponentialSmoothing(train_split, trend = trend, seasonal = seasonal, \n",
    "                                             seasonal_periods = seasonal_periods,\n",
    "                                             initialization_method = initialization_method,\n",
    "                                             use_boxcox = use_boxcox).fit()\n",
    "                predictions = model.predict(start=len(train_split), end=len(train_split) + len(val_split) - 1)\n",
    "                mse = mean_squared_error(val_split, predictions)\n",
    "                mse_sum += mse\n",
    "                # Store the predictions and index for the last validation split\n",
    "                if len(val_index) > 0 and val_index[0] == len(self.data) - len(val_split):\n",
    "                    best_val_predictions = predictions\n",
    "                    best_val_index = val_index\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                return float('inf')  # Return a very low value if an error occurs\n",
    "\n",
    "        avg_mse = mse_sum / n_splits\n",
    "        # Store the best predictions and index within the trial object for later retrieval\n",
    "        if avg_mse < best_mse:\n",
    "            trial.set_user_attr('best_mse', avg_mse)\n",
    "            trial.set_user_attr(\"best_val_predictions\", best_val_predictions)\n",
    "            trial.set_user_attr(\"best_val_index\", best_val_index)\n",
    "        return avg_mse\n",
    "    \n",
    "    def train(self):\n",
    "        # Check stationarity and apply log transformation if needed\n",
    "        if not self.check_stationarity(self.data):\n",
    "            print(\"Series is not stationary. Applying log transformation...\")\n",
    "            self.data = self.log_transform(self.data)\n",
    "                \n",
    "            # Create an Optuna study\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        # Define an initial best_mse as infinity\n",
    "        initial_best_mse = float('inf')\n",
    "        # Define the objective function with an initial best_mse\n",
    "        def objective_with_initial_best_mse(trial):\n",
    "            trial.set_user_attr(\"best_mse\", initial_best_mse)\n",
    "            return self.objective(trial)\n",
    "\n",
    "        study.optimize(objective_with_initial_best_mse, n_trials=75)  # Number of trials can be adjusted\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_mse = study.best_value  # Access custom attributes returned from objective function\n",
    "\n",
    "        # Retrieve the best trial\n",
    "        best_trial = study.best_trial\n",
    "\n",
    "        # Store the best validation predictions and index\n",
    "        self.last_val_predictions = best_trial.user_attrs[\"best_val_predictions\"]\n",
    "        self.last_val_index = best_trial.user_attrs[\"best_val_index\"]\n",
    "\n",
    "        \n",
    "        print(f\"Best MSE score: {best_mse:.4f}\")\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "        # Fit the best model on the entire dataset\n",
    "        try:\n",
    "            self.trained_model = ExponentialSmoothing(self.data, trend = best_params['trend'], seasonal = best_params['seasonal'],\n",
    "                                                        seasonal_periods = best_params['seasonal_periods'], \n",
    "                                                        initialization_method = best_params['initialization_method'],\n",
    "                                                        use_boxcox = best_params['use_boxcox']).fit()\n",
    "            print(f'Model training successful')\n",
    "        except Exception as e:\n",
    "            print(f'Model training failed with the error message: {e}')\n",
    "            \n",
    "    def forecast(self, forecast_days):\n",
    "        #Forecast next forecast_period days\n",
    "        start = len(self.data)\n",
    "        end = start + forecast_days - 1\n",
    "        forecast_prices = self.trained_model.predict(start=start, end=end)\n",
    "       # Reverse log transformation if applied\n",
    "        if not self.stationary:\n",
    "            self.data = np.exp(self.data)\n",
    "            forecast_prices = np.exp(forecast_prices)\n",
    "            if self.last_val_predictions is not None and self.last_val_index is not None:\n",
    "                self.last_val_predictions = np.exp(self.last_val_predictions)\n",
    "\n",
    "        # Plot the data\n",
    "        # Create date range for forecasted data\n",
    "        forecast_dates = pd.date_range(start=self.data.index[-1] + pd.Timedelta(days=1), periods=forecast_days, freq='D')\n",
    "        # Create figure and axis\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True, figsize=(16, 8), gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "        # Create candlestick data\n",
    "        candlestick_data = pd.DataFrame({\n",
    "            'Date': self.data.index,\n",
    "            'Open': self.open,\n",
    "            'Close': self.data,\n",
    "            'High': self.high,\n",
    "            'Low': self.low\n",
    "        })\n",
    "        # Plot the candlestick data with decreased transparency\n",
    "        for idx, row in candlestick_data.iterrows():\n",
    "            date_num = mdates.date2num(row['Date'])\n",
    "            if row['Close'] >= row['Open']:\n",
    "                color = 'green'\n",
    "                lower = row['Open']\n",
    "                height = row['Close'] - row['Open']\n",
    "            else:\n",
    "                color = 'red'\n",
    "                lower = row['Close']\n",
    "                height = row['Open'] - row['Close']\n",
    "            \n",
    "            # Draw high and low lines (wicks) outside the rectangle\n",
    "            ax1.vlines(date_num, row['Low'], lower, color=color, alpha=0.5, linewidth=0.5)\n",
    "            ax1.vlines(date_num, lower + height, row['High'], color=color, alpha=0.5, linewidth=0.5)\n",
    "            \n",
    "            # Draw the rectangle (candlestick body)\n",
    "            ax1.add_patch(mpatches.Rectangle((date_num - 0.5, lower), 1, height, edgecolor=color, facecolor=color, alpha=1, linewidth=1))\n",
    "        \n",
    "        # Plot the price data\n",
    "        ax1.plot(self.data.index, self.data, label='Historical Data', color='gray', linewidth=1, alpha=0.6)\n",
    "        ax1.plot(forecast_dates, forecast_prices, label='Forecasted Prices', color='black', linewidth=1.5, linestyle = '-')\n",
    "        ax1.set_title(f'Model: {self.model_type} \\n Symbol: {self.symbol_name}', weight = 'bold', fontsize = 16)\n",
    "        ax1.set_ylabel('Price', weight = 'bold', fontsize = 15)\n",
    "        ax1.grid(True, alpha = 0.3)\n",
    "\n",
    "        # Plot the last validation split predictions if available\n",
    "        if self.show_backtest:\n",
    "            if self.last_val_predictions is not None and self.last_val_index is not None:\n",
    "                ax1.plot(self.data.index[self.last_val_index], self.last_val_predictions, label='Backtest Predictions', color='dimgray', linewidth=1.5, linestyle='-')\n",
    "\n",
    "        ax1.legend(loc='upper left')\n",
    "        # Plot the volume data\n",
    "        volume_colors = np.where(self.data.diff() >= 0, 'green', 'red')\n",
    "        ax2.bar(self.data.index, self.volume, color=volume_colors, alpha=0.6)\n",
    "        ax2.set_ylabel('Volume', weight = 'bold', fontsize = 15)\n",
    "        ax2.set_xlabel('Time', weight = 'bold', fontsize = 15)\n",
    "        ax2.grid(True, alpha = 0.3)\n",
    "        \n",
    "        # Save plot to a bytes buffer\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        plot_data = base64.b64encode(buf.read()).decode('utf-8')\n",
    "        buf.close()\n",
    "        plt.show()\n",
    "        #plt.close(fig)  # Close the plot to free up resources\n",
    "\n",
    "        return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'AMZN'\n",
    "data_length = 1800\n",
    "forecast_days = 90\n",
    "#Set start and end date\n",
    "now = datetime.now()\n",
    "start_date =  (now - timedelta(days = data_length)).strftime(\"%Y-%m-%d\")\n",
    "end_date = now.strftime(\"%Y-%m-%d\")\n",
    "data = fetch_data(symbol, start_date, end_date)\n",
    "model = HWES_model(data, symbol)\n",
    "model.train()\n",
    "model.forecast(forecast_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 ARCH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARCH_model(Model):\n",
    "    def prepare_data(self, data):\n",
    "        data = data.dropna()\n",
    "        if not isinstance(data.index, pd.DatetimeIndex):\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "        if not data.index.freq:\n",
    "            # Attempt to infer the frequency\n",
    "            inferred_freq = pd.infer_freq(data.index)\n",
    "            if inferred_freq:\n",
    "                data.index.freq = inferred_freq\n",
    "            else:\n",
    "                # Handle the case where frequency cannot be inferred\n",
    "                # For example, you might decide to use a default frequency or handle this as an exception\n",
    "                print(\"Unable to infer frequency for the datetime index.\")\n",
    "\n",
    "        data.index.length = len(data)\n",
    "        return data\n",
    "    \n",
    "    def __init__(self, data, symbol_name):\n",
    "        data = self.prepare_data(data)\n",
    "        super().__init__(data = data['Close'], open = data['Open'], high = data['High'], low = data['Low'], volume = data['Volume'], symbol_name = symbol_name)\n",
    "        self.trained_model = None\n",
    "        self.model_type = 'Autoregressive Conditional Heteroskedasticity'\n",
    "        self.stationary = False\n",
    "        self.show_backtest = True\n",
    "\n",
    "    def check_stationarity(self, series, alpha=0.05):\n",
    "        series = series.dropna()\n",
    "        result = adfuller(series)\n",
    "        p_value = result[1]\n",
    "        self.stationary = p_value < alpha\n",
    "        return self.stationary \n",
    "\n",
    "    def log_transform(self, series):\n",
    "        return np.log(series).dropna()\n",
    "    \n",
    "    def objective(self, trial):\n",
    "\n",
    "        # Define range of parameters\n",
    "        rescale = trial.suggest_categorical('rescale', [True, False])\n",
    "        min_lag = 1\n",
    "        max_lag = int(np.sqrt(len(self.data))) if len(self.data) >= 20 else len(self.data) // 2  # Ensure a practical upper bound for small datasets\n",
    "        lags = trial.suggest_int('lags', min_lag, max_lag)\n",
    "        p = trial.suggest_int('p', 1, 5)\n",
    "        q = trial.suggest_int('q', 1, 5)\n",
    "        vol = trial.suggest_categorical('vol', ['GARCH', 'ARCH', 'EGARCH', 'FIGARCH', \n",
    "                                                'APARCH', 'HARCH'])\n",
    "        mean = trial.suggest_categorical('mean', ['Zero', 'LS', 'AR', \n",
    "                                                  'ARX', 'HAR', 'HARX'])\n",
    "        mse_sum = 0\n",
    "        n_splits = 2\n",
    "        best_val_predictions = None\n",
    "        best_val_index = None\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "        # Retrieve the current best_mse from user attributes\n",
    "        best_mse = trial.user_attrs.get('best_mse', float('inf'))\n",
    "\n",
    "\n",
    "        for train_index, val_index in tscv.split(self.data):\n",
    "            train_split, val_split = self.data.iloc[train_index], self.data.iloc[val_index]\n",
    "            try:\n",
    "                model = arch_model(train_split, p = p, q = q, mean = mean, lags = lags, \n",
    "                                   rescale = rescale, vol = vol).fit(disp = 'off')\n",
    "                predictions = model.forecast(horizon=len(val_split)).mean.values[-1, :]\n",
    "                mse = mean_squared_error(val_split, predictions)\n",
    "                mse_sum += mse\n",
    "                # Store the predictions and index for the last validation split\n",
    "                if len(val_index) > 0 and val_index[0] == len(self.data) - len(val_split):\n",
    "                    best_val_predictions = predictions\n",
    "                    best_val_index = val_index\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                return float('inf')  # Return a very low value if an error occurs\n",
    "\n",
    "        avg_mse = mse_sum / n_splits\n",
    "        if avg_mse<best_mse:\n",
    "            # Store the best predictions and index within the trial object for later retrieval\n",
    "            trial.set_user_attr(\"best_mse\", avg_mse)\n",
    "            trial.set_user_attr(\"best_val_predictions\", best_val_predictions)\n",
    "            trial.set_user_attr(\"best_val_index\", best_val_index)\n",
    "\n",
    "        return avg_mse\n",
    "    \n",
    "    def train(self):\n",
    "            # Check stationarity and apply log transformation if needed\n",
    "            if not self.check_stationarity(self.data):\n",
    "                print(\"Series is not stationary. Applying log transformation...\")\n",
    "                self.data = self.log_transform(self.data)\n",
    "                    \n",
    "            # Create an Optuna study\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "\n",
    "            # Define an initial best_mse as infinity\n",
    "            initial_best_mse = float('inf')\n",
    "            # Define the objective function with an initial best_mse\n",
    "            def objective_with_initial_best_mse(trial):\n",
    "                trial.set_user_attr(\"best_mse\", initial_best_mse)\n",
    "                return self.objective(trial)\n",
    "\n",
    "            study.optimize(objective_with_initial_best_mse, n_trials=75)  # Number of trials can be adjusted\n",
    "\n",
    "            best_params = study.best_params\n",
    "            best_mse = study.best_value  # Access custom attributes returned from objective function\n",
    "\n",
    "            # Retrieve the best trial\n",
    "            best_trial = study.best_trial\n",
    "\n",
    "            # Store the best validation predictions and index\n",
    "            self.last_val_predictions = best_trial.user_attrs[\"best_val_predictions\"]\n",
    "            self.last_val_index = best_trial.user_attrs[\"best_val_index\"]\n",
    "\n",
    "            \n",
    "            print(f\"Best MSE score: {best_mse:.4f}\")\n",
    "            print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "            # Fit the best model on the entire dataset\n",
    "            try:\n",
    "                self.trained_model = arch_model(self.data, p = best_params['p'], q = best_params['q'], \n",
    "                                                mean = best_params['mean'],lags =  best_params['lags'],\n",
    "                                                rescale =  best_params['rescale'], vol = best_params['vol']).fit(disp = 'off')\n",
    "                print(f'Model training successful')\n",
    "            except Exception as e:\n",
    "                print(f'Model training failed with the error message: {e}')\n",
    "            \n",
    "    def forecast(self, forecast_days):\n",
    "        #Forecast next forecast_period days\n",
    "        start = len(self.data)\n",
    "        end = start + forecast_days - 1\n",
    "        forecast_prices = self.trained_model.forecast(horizon=forecast_days).mean.values[-1, :]\n",
    "       # Reverse log transformation if applied\n",
    "        if not self.stationary:\n",
    "            self.data = np.exp(self.data)\n",
    "            forecast_prices = np.exp(forecast_prices)\n",
    "            if self.last_val_predictions is not None and self.last_val_index is not None:\n",
    "                self.last_val_predictions = np.exp(self.last_val_predictions)\n",
    "\n",
    "        # Plot the data\n",
    "        # Create date range for forecasted data\n",
    "        forecast_dates = pd.date_range(start=self.data.index[-1] + pd.Timedelta(days=1), periods=forecast_days, freq='D')\n",
    "        # Create figure and axis\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True, figsize=(16, 8), gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "        # Create candlestick data\n",
    "        candlestick_data = pd.DataFrame({\n",
    "            'Date': self.data.index,\n",
    "            'Open': self.open,\n",
    "            'Close': self.data,\n",
    "            'High': self.high,\n",
    "            'Low': self.low\n",
    "        })\n",
    "        # Plot the candlestick data with decreased transparency\n",
    "        for idx, row in candlestick_data.iterrows():\n",
    "            date_num = mdates.date2num(row['Date'])\n",
    "            if row['Close'] >= row['Open']:\n",
    "                color = 'green'\n",
    "                lower = row['Open']\n",
    "                height = row['Close'] - row['Open']\n",
    "            else:\n",
    "                color = 'red'\n",
    "                lower = row['Close']\n",
    "                height = row['Open'] - row['Close']\n",
    "            \n",
    "            # Draw high and low lines (wicks) outside the rectangle\n",
    "            ax1.vlines(date_num, row['Low'], lower, color=color, alpha=0.5, linewidth=0.5)\n",
    "            ax1.vlines(date_num, lower + height, row['High'], color=color, alpha=0.5, linewidth=0.5)\n",
    "            \n",
    "            # Draw the rectangle (candlestick body)\n",
    "            ax1.add_patch(mpatches.Rectangle((date_num - 0.5, lower), 1, height, edgecolor=color, facecolor=color, alpha=1, linewidth=1))\n",
    "        \n",
    "        # Plot the price data\n",
    "        ax1.plot(self.data.index, self.data, label='Historical Data', color='gray', linewidth=1, alpha=0.6)\n",
    "        ax1.plot(forecast_dates, forecast_prices, label='Forecasted Prices', color='black', linewidth=1.5, linestyle = '-')\n",
    "        ax1.set_title(f'Model: {self.model_type} \\n Symbol: {self.symbol_name}', weight = 'bold', fontsize = 16)\n",
    "        ax1.set_ylabel('Price', weight = 'bold', fontsize = 15)\n",
    "        ax1.grid(True, alpha = 0.3)\n",
    "\n",
    "        # Plot the last validation split predictions if available\n",
    "        if self.show_backtest:\n",
    "            if self.last_val_predictions is not None and self.last_val_index is not None:\n",
    "                ax1.plot(self.data.index[self.last_val_index], self.last_val_predictions, label='Backtest Predictions', color='dimgray', linewidth=1.5, linestyle='-')\n",
    "\n",
    "        ax1.legend(loc='upper left')\n",
    "        # Plot the volume data\n",
    "        volume_colors = np.where(self.data.diff() >= 0, 'green', 'red')\n",
    "        ax2.bar(self.data.index, self.volume, color=volume_colors, alpha=0.6)\n",
    "        ax2.set_ylabel('Volume', weight = 'bold', fontsize = 15)\n",
    "        ax2.set_xlabel('Time', weight = 'bold', fontsize = 15)\n",
    "        ax2.grid(True, alpha = 0.3)\n",
    "        \n",
    "        # Save plot to a bytes buffer\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        plot_data = base64.b64encode(buf.read()).decode('utf-8')\n",
    "        buf.close()\n",
    "        plt.show()\n",
    "        #plt.close(fig)  # Close the plot to free up resources\n",
    "\n",
    "        return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'AMZN'\n",
    "data_length = 1800\n",
    "forecast_days = 90\n",
    "#Set start and end date\n",
    "now = datetime.now()\n",
    "start_date =  (now - timedelta(days = data_length)).strftime(\"%Y-%m-%d\")\n",
    "end_date = now.strftime(\"%Y-%m-%d\")\n",
    "data = fetch_data(symbol, start_date, end_date)\n",
    "model = GARCH_model(data, symbol)\n",
    "model.train()\n",
    "model.forecast(forecast_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_type, data, symbol_name):\n",
    "    if model_type == 'AR':\n",
    "        return AR_model(data, symbol_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Database for User and Model Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Users Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True, autoincrement=True)\n",
    "    email = db.Column(db.String(120), unique=True, nullable=False)\n",
    "    password = db.Column(db.String(255), nullable=False)\n",
    "    account_type = db.Column(db.String(20), nullable=False)  # 'basic' or 'premium'\n",
    "    symbols = db.relationship('Symbol', backref='user', lazy=True)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<User: {self.email} - Account Type: {self.account_type}>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Symbols Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Symbol(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True, autoincrement=True)\n",
    "    name = db.Column(db.String(50), nullable=False, unique=False)\n",
    "    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<Symbol: {self.name}>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Trained Models Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainedModels(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True, autoincrement=True)\n",
    "    symbol = db.Column(db.String(20), nullable=False)\n",
    "    model_type = db.Column(db.String(50), nullable=False)\n",
    "    start_date = db.Column(db.String(50), nullable=False)\n",
    "    end_date = db.Column(db.String(50), nullable=False)  # Nullable if model is ongoing\n",
    "    model_obj = db.Column(db.Text)  # Serialized model data or file path\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<Model: {self.model_type} - Symbol: {self.symbol} - Start Date: {self.start_date} - End Date: {self.end_date}>'\n",
    "\n",
    "    def save_trained_model(self, model_obj):\n",
    "        \"\"\"\n",
    "        Save the trained model to the database.\n",
    "        \"\"\"\n",
    "        self.model_obj = pickle.dumps(model_obj)\n",
    "        db.session.add(self)\n",
    "        db.session.commit()\n",
    "    \n",
    "    @classmethod\n",
    "    def load_trained_model(self, model_type, start_date, end_date, symbol):\n",
    "        model_entry = TrainedModels.query.filter_by(\n",
    "                        model_type=model_type,\n",
    "                        start_date=start_date,\n",
    "                        end_date=end_date,\n",
    "                        symbol=symbol\n",
    "                    ).first()\n",
    "        if model_entry:\n",
    "            return pickle.loads(model_entry.model_obj)\n",
    "        else:\n",
    "            return None\n",
    "    @classmethod\n",
    "    def delete_all_models(self):\n",
    "        try:\n",
    "            # Delete all entries from the TrainedModels table\n",
    "            TrainedModels.query.delete()\n",
    "            db.session.commit()\n",
    "            print(\"All models are deleted successfully.\")\n",
    "        except Exception as e:\n",
    "            db.session.rollback()\n",
    "            print(f\"Error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Temporary Password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporaryPassword(db.Model):\n",
    "    email = db.Column(db.String(120), primary_key=True, nullable=False)\n",
    "    temp_password = db.Column(db.String(255), nullable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('login.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Login "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/login', methods=['GET', 'POST'])\n",
    "def login():\n",
    "\n",
    "    if request.method == 'GET':\n",
    "        # Serve the signup and forgot password page\n",
    "        return render_template('login.html')\n",
    "    \n",
    "    if request.method == 'POST':\n",
    "        data = request.json\n",
    "        email = data.get('email')\n",
    "        password = data.get('password')\n",
    "        \n",
    "        user = User.query.filter_by(email=email).first()\n",
    "\n",
    "        if user and check_password_hash(user.password, password):\n",
    "            # Store user ID in session\n",
    "            session['user_id'] = user.id\n",
    "            session['email'] = user.email  # Store the email in the session\n",
    "            return jsonify({'message': 'Login successful!', 'redirect': url_for('main')}), 200\n",
    "        else:\n",
    "            return jsonify({'message': 'Invalid credentials'}), 401"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Signup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_password(password):\n",
    "    \"\"\"Check if the password is valid based on the criteria.\"\"\"\n",
    "    if len(password) < 8: # Check password length\n",
    "        return False\n",
    "    if not re.search(r'[A-Z]', password):  # Check for uppercase letter\n",
    "        return False\n",
    "    if not re.search(r'[a-z]', password):  # Check for lowercase letter\n",
    "        return False\n",
    "    if not re.search(r'[0-9]', password):  # Check for number\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/signup', methods=['GET', 'POST'])\n",
    "def signup():\n",
    "    if request.method == 'GET':\n",
    "        # Serve the signup page\n",
    "        return render_template('signup.html')\n",
    "    \n",
    "    if request.method == 'POST':\n",
    "        data = request.json\n",
    "        if not data:\n",
    "            return jsonify({'message': 'No data provided'}), 400\n",
    "        \n",
    "        email = data.get('email')\n",
    "        password = data.get('password')\n",
    "        password_re = data.get('password_re')\n",
    "        \n",
    "        # Check if all fields are present\n",
    "        if not email or not password or not password_re:\n",
    "            return jsonify({'message': 'All fields are required.'}), 400\n",
    "\n",
    "        # Check if email already exists\n",
    "        existing_user = User.query.filter_by(email=email).first()\n",
    "        if existing_user:\n",
    "            return jsonify({'message': 'User already exists.'}), 400\n",
    "\n",
    "        # Check if passwords match\n",
    "        if password != password_re:\n",
    "            return jsonify({'message': 'Passwords do not match.'}), 400\n",
    "        \n",
    "        # Validate password strength\n",
    "        if not is_valid_password(password):\n",
    "            return jsonify({'message': 'Password must be at least 8 characters long, include uppercase letters, lowercase letters, and numbers.'}), 400\n",
    "        \n",
    "        # Create new user\n",
    "        hashed_password = generate_password_hash(password, method='pbkdf2:sha256')\n",
    "\n",
    "        # Creating an empty list of symbols for the new user is implicit because we start with no symbols related to the user\n",
    "        new_user = User(email=email, password=hashed_password, account_type='basic')\n",
    "        try:\n",
    "            db.session.add(new_user)\n",
    "            db.session.commit()\n",
    "            return jsonify({'message': 'Sign-up successful!', 'redirect': url_for('login')}), 200\n",
    "        except Exception as e:\n",
    "            return jsonify({'message': 'Error creating user.'}), 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Request E-mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temporary_password(length=8):\n",
    "    letters_and_digits = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(letters_and_digits) for i in range(length))\n",
    "\n",
    "def send_email(email, temporary_password):\n",
    "    # Placeholder function to simulate sending an email\n",
    "    # Implement actual email sending logic here\n",
    "    print(f\"Sending temporary password to {email}: {temporary_password}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/request_email', methods=['GET', 'POST'])\n",
    "def request_email():\n",
    "    if request.method == 'POST':\n",
    "        data = request.get_json()\n",
    "        email = data.get('email')\n",
    "        \n",
    "        # Check if the email exists in the User database\n",
    "        user = User.query.filter_by(email=email).first()\n",
    "        if user:  \n",
    "            # Generate a temporary password\n",
    "            temp_password = generate_temporary_password()\n",
    "            # Check if a temporary password entry already exists for the email\n",
    "            temp_password_entry = TemporaryPassword.query.filter_by(email=email).first()\n",
    "            if temp_password_entry:\n",
    "                db.session.delete(temp_password_entry)\n",
    "                db.session.commit()\n",
    "\n",
    "            temp_password_entry = TemporaryPassword(email=email, temp_password=temp_password)\n",
    "            db.session.add(temp_password_entry)\n",
    "            db.session.commit()\n",
    "            \n",
    "            # Send a temporary password\n",
    "            send_email(email, temp_password)\n",
    "\n",
    "            # Store the email in a session to use in forgot password section\n",
    "            session['resetEmail'] = email\n",
    "            \n",
    "            return jsonify({'success': True, 'message': 'Temporary password sent to your email'})\n",
    "        else:\n",
    "            return jsonify({'success': False, 'message': 'Email not found'})\n",
    "\n",
    "    return render_template('request_email.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Forgot Passward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/forgot_password', methods=['GET', 'POST'])\n",
    "def forgot_password():\n",
    "    if request.method == 'POST':\n",
    "        data = request.get_json()\n",
    "        temp_password = data.get('tempPassword')\n",
    "        \n",
    "        #Retrieve email from session\n",
    "        email = session.get('resetEmail')\n",
    "\n",
    "        if not email:\n",
    "            return jsonify({'success': False, 'message': 'No email found in session. Please request an email first.'})\n",
    "\n",
    "        # Check the temporary password in the database\n",
    "        temp_password_entry = TemporaryPassword.query.filter_by(email=email, temp_password=temp_password).first()\n",
    "        if temp_password_entry:\n",
    "            # Temporary password matches\n",
    "            return jsonify({'success': True, 'message': 'Temporary password verified. Proceed to reset password.'})\n",
    "        else:\n",
    "            return jsonify({'success': False, 'message': 'Invalid temporary password'})\n",
    "\n",
    "    return render_template('forgot_password.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Reset Password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/reset_password', methods=['GET', 'POST'])\n",
    "def reset_password():\n",
    "    if request.method == 'POST':\n",
    "        data = request.get_json()\n",
    "        new_password = data.get('newPassword')\n",
    "        new_password_re = data.get('newPassword_re')\n",
    "\n",
    "        # Retrieve the email from session\n",
    "        email = session.get('resetEmail')\n",
    "\n",
    "        if not email:\n",
    "            return jsonify({'success': False, 'message': 'No email found in session. Please request an email first.'})\n",
    "\n",
    "        if new_password != new_password_re:\n",
    "            return jsonify({'success': False, 'message': 'Passwords do not match'})\n",
    "        \n",
    "        # Validate password strength\n",
    "        if not is_valid_password(new_password):\n",
    "            return jsonify({'message': 'Password must be at least 8 characters long, include uppercase letters, lowercase letters, and numbers.'}), 400\n",
    "\n",
    "        # Fetch the user from the database\n",
    "        user = User.query.filter_by(email=email).first()\n",
    "        if user:\n",
    "            # Check if the new password is different from the current password\n",
    "            if check_password_hash(user.password, new_password):\n",
    "                return jsonify({'success': False, 'message': 'New password must be different from the old password'})\n",
    "            \n",
    "            # Update the password\n",
    "            hashed_new_password = generate_password_hash(new_password, method='pbkdf2:sha256')\n",
    "            user.password = hashed_new_password\n",
    "            db.session.commit()\n",
    "            return jsonify({'success': True, 'message': 'Password successfully updated. You can now login with your new password.'})\n",
    "        else:\n",
    "            return jsonify({'success': False, 'message': 'User not found'})\n",
    "\n",
    "    # Ensure session email is set, otherwise redirect to request_email\n",
    "    if not session.get('resetEmail'):\n",
    "        return jsonify({'redirect': '/request_email', 'message': 'No email found in session. Please request an email first.'})\n",
    "\n",
    "    return render_template('reset_password.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/forecaster', methods=['GET', 'POST']) # Forecaster Button \n",
    "def main():\n",
    "    # Data fetching\n",
    "    return render_template('main.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/logout', methods=['GET', 'POST']) # Logout Button\n",
    "def logout():\n",
    "    # Perform logout operations if needed\n",
    "    return redirect(url_for('login'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/my_profile', methods=['GET'])\n",
    "def my_profile():\n",
    "    user_id = session['user_id']\n",
    "    user = User.query.get(user_id)  # Fetch the user by ID\n",
    "    \n",
    "    return render_template('my_profile.html', user=user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/about_models', methods=['GET', 'POST']) # About Models Button\n",
    "def about_models():\n",
    "    return render_template('about_models.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/get_symbols', methods=['GET']) # Getter method to fetch symbols from the users account\n",
    "def get_symbols():\n",
    "    user_id = session.get('user_id')\n",
    "    if not user_id:\n",
    "        return jsonify({'success': False, 'message': 'User not logged in'}), 401\n",
    "\n",
    "    symbols = Symbol.query.filter_by(user_id=user_id).all()\n",
    "    symbol_list = [symbol.name for symbol in symbols]\n",
    "\n",
    "    return jsonify({'symbols': symbol_list}), 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(symbol, start_date, end_date): # fetch data based on user preference\n",
    "    data = None\n",
    "    try:\n",
    "        # Fetch historical price data\n",
    "        df = yf.download(symbol, start=start_date, end=end_date)\n",
    "\n",
    "        # Drop 'Adj Close' column if present\n",
    "        if 'Adj Close' in df.columns:\n",
    "            df.drop(columns=['Adj Close'], inplace=True)\n",
    "\n",
    "        # Store the dataframe with technical indicators\n",
    "        data = df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def check_symbol_existence(symbol): #check the existence of symbol from yahoo finance\n",
    "    try:\n",
    "        stock = yf.Ticker(symbol)\n",
    "        data = stock.history(period=\"1d\")\n",
    "        return not data.empty\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for symbol {symbol}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_days(input_length):\n",
    "    days = 0\n",
    "    if input_length == '1_week':\n",
    "        days = 7\n",
    "    elif input_length == '1_month':\n",
    "        days = 30\n",
    "    elif input_length == '2_months':\n",
    "        days = 60\n",
    "    elif input_length == '3_months':\n",
    "        days = 90\n",
    "    elif input_length == '4_months':\n",
    "        days = 120\n",
    "    elif input_length == '5_months':\n",
    "        days = 150\n",
    "    elif input_length == '6_months':\n",
    "        days = 180\n",
    "    elif input_length == '9_months':\n",
    "        days = 270\n",
    "    elif input_length == '1_year':\n",
    "        days = 360\n",
    "    elif input_length == '2_years':\n",
    "        days = 720\n",
    "    elif input_length == '3_years':\n",
    "        days = 1080\n",
    "    elif input_length == '4_years':\n",
    "        days = 1440\n",
    "    elif input_length == '5_years':\n",
    "        days = 1800\n",
    "        \n",
    "    return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/add_symbol', methods=['POST'])\n",
    "def add_symbol(): # add symbol to the db\n",
    "    user_id = session.get('user_id')\n",
    "    if not user_id:\n",
    "        return jsonify({'success': False, 'message': 'User not logged in'}), 401\n",
    "\n",
    "    data = request.json\n",
    "    symbol_name = data.get('symbol')\n",
    "    if not symbol_name:\n",
    "        return jsonify({'success': False, 'message': 'Symbol name not provided'}), 400\n",
    "\n",
    "    if not check_symbol_existence(symbol_name):\n",
    "        return jsonify({'success': False, 'message': 'Symbol does not exist'}), 404\n",
    "\n",
    "    existing_symbol = Symbol.query.filter_by(name=symbol_name, user_id=user_id).first()\n",
    "    if existing_symbol:\n",
    "        return jsonify({'success': False, 'message': 'Symbol already added'}), 400\n",
    "    \n",
    "    new_symbol = Symbol(name=symbol_name, user_id=user_id)\n",
    "    db.session.add(new_symbol)\n",
    "    db.session.commit()\n",
    "\n",
    "    return jsonify({'success': True}), 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/delete_symbol', methods=['POST'])\n",
    "def delete_symbol(): # delete symbol from the db\n",
    "    data = request.json\n",
    "    if not data or 'symbol' not in data:\n",
    "        return jsonify({'message': 'Invalid request'}), 400\n",
    "\n",
    "    symbol_name = data['symbol']\n",
    "    user_id = session.get('user_id')\n",
    "\n",
    "    # Find and delete the symbol\n",
    "    symbol = Symbol.query.filter_by(name=symbol_name, user_id=user_id).first()\n",
    "    if symbol:\n",
    "        db.session.delete(symbol)\n",
    "        db.session.commit()\n",
    "        return jsonify({'success': True}), 200\n",
    "    else:\n",
    "        return jsonify({'message': 'Symbol not found'}), 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/predict', methods=['GET', 'POST'])\n",
    "def predict(): # predict button\n",
    "    try:\n",
    "        data = request.json\n",
    "        symbol = data['symbol']\n",
    "        data_length = convert_to_days(data['data_length'])\n",
    "        forecast_days = convert_to_days(data['forecast_days'])\n",
    "        model_type = data['model_type']\n",
    "        #Set start and end date\n",
    "        now = datetime.now()\n",
    "        start_date =  (now - timedelta(days = data_length)).strftime(\"%Y-%m-%d\")\n",
    "        end_date = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        #Check if model already exists\n",
    "        model_obj = TrainedModels.load_trained_model(model_type=model_type, start_date=start_date, end_date=end_date, symbol=symbol)\n",
    "        if not model_obj:  # if the model is already trained avoid re-training it.\n",
    "            data = fetch_data(symbol, start_date, end_date)\n",
    "            model_obj = create_model(model_type, data, symbol)\n",
    "            model_obj.train()\n",
    "            # Create a new instance of Model\n",
    "            new_model = TrainedModels(symbol= symbol, model_type= model_type, start_date= start_date, end_date= end_date)\n",
    "            # Save the trained model to the database\n",
    "            new_model.save_trained_model(model_obj)\n",
    "        \n",
    "        # Forecast\n",
    "        plot_data = model_obj.forecast(forecast_days=forecast_days)\n",
    "        return jsonify({'plot': plot_data}), 200\n",
    "\n",
    "    except KeyError as e:\n",
    "        return jsonify({'error': f'Missing key: {e.args[0]}'}, 400)\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 My Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/delete_account', methods=['POST'])\n",
    "def delete_account():\n",
    "    if 'user_id' not in session:\n",
    "        #flash('You need to log in first.', 'warning')\n",
    "        return redirect(url_for('login'))\n",
    "\n",
    "    user_id = session['user_id']\n",
    "    user = User.query.get(user_id)  # Fetch user by ID\n",
    "    if user:\n",
    "        # Step 1: Delete all symbols associated with the user\n",
    "        Symbol.query.filter_by(user_id=user.id).delete()\n",
    "\n",
    "        # Step 2: Delete the user from the User table\n",
    "        db.session.delete(user)\n",
    "\n",
    "        # Step 3: Commit the transaction to apply changes to the database\n",
    "        db.session.commit()\n",
    "\n",
    "        # Step 4: Remove the user from the session since the account is deleted\n",
    "        session.pop('user_id', None)\n",
    "        session.pop('email', None)  # Clean up the email as well\n",
    "\n",
    "        #flash('Your account has been deleted successfully.', 'success')\n",
    "        return redirect(url_for('login'))  # Redirect to login page\n",
    "    else:\n",
    "        #flash('Account deletion failed.', 'danger')\n",
    "        return redirect(url_for('my_profile'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/upgrade_to_premium', methods=['POST'])\n",
    "def upgrade_to_premium():\n",
    "    if 'user_id' not in session:\n",
    "        #flash('You must be logged in to upgrade your account.', 'danger')\n",
    "        return redirect(url_for('login'))\n",
    "\n",
    "    user_id = session['user_id']\n",
    "    user = User.query.get(user_id)  # Fetch user by ID\n",
    "\n",
    "    if user:\n",
    "        user.account_type = 'premium'\n",
    "        db.session.commit()\n",
    "        #flash('You have been upgraded to a premium account.', 'success')\n",
    "        return redirect(url_for('my_profile'))\n",
    "    else:\n",
    "        #flash('Account upgrade failed.', 'danger')\n",
    "        return redirect(url_for('my_profile'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/downgrade_to_basic', methods=['POST'])\n",
    "def downgrade_to_basic():\n",
    "    if 'user_id' not in session:\n",
    "        #flash('You must be logged in to downgrade your account.', 'danger')\n",
    "        return redirect(url_for('login'))\n",
    "\n",
    "    user_id = session['user_id']\n",
    "    user = User.query.get(user_id)\n",
    "    \n",
    "    if user and user.account_type == 'premium':\n",
    "        user.account_type = 'basic'\n",
    "        db.session.commit()\n",
    "        #flash('You have been downgraded to a basic account.', 'success')\n",
    "        return redirect(url_for('my_profile'))\n",
    "    else:\n",
    "        #flash('Account downgrade failed or already basic.', 'danger')\n",
    "        return redirect(url_for('my_profile'))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 About Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 App Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    with app.app_context():\n",
    "        db.create_all()\n",
    "    app.run(debug=True, use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
